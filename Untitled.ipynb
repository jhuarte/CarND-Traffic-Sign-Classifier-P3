{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"data/train.p\"\n",
    "validation_file= \"data/valid.p\"\n",
    "testing_file = \"data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "x_train, y_train = train['features'], train['labels']\n",
    "x_valid, y_valid = valid['features'], valid['labels']\n",
    "x_test, y_test = test['features'], test['labels']\n",
    "\n",
    "assert(len(x_train) == len(y_train))\n",
    "assert(len(x_valid) == len(y_valid))\n",
    "assert(len(x_test) == len(y_test))\n",
    "\n",
    "x_train, x_test, x_valid = np.array(x_train, np.float32), np.array(x_test, np.float32), np.array(x_valid, np.float32)\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(x_train)\n",
    "\n",
    "# Number of validation examples\n",
    "n_validation = len(x_valid)\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(x_test)\n",
    "\n",
    "# Shape of image\n",
    "image_shape = x_train[0].shape\n",
    "\n",
    "# Unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "signals = pd.read_csv('signnames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Normalize the dataset (data)\n",
    "    \n",
    "    Parameters:\n",
    "        data: dataset\n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    return (data - 128.) / 128.\n",
    "\n",
    "x_train = np.sum(x_train/3, axis=3, keepdims=True)\n",
    "x_test = np.sum(x_test/3, axis=3, keepdims=True)\n",
    "x_valid = np.sum(x_valid/3, axis=3, keepdims=True)\n",
    "\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)\n",
    "x_valid = normalize(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters.\n",
    "conv1_filters = 6 # number of filters for 1st conv layer.\n",
    "conv2_filters = 16 # number of filters for 2nd conv layer.\n",
    "fc1_units = 120 # number of neurons for 1st fully-connected layer.\n",
    "fc2_units = 84 # number of neurons for 1st fully-connected layer.\n",
    "num_classes = n_classes\n",
    "\n",
    "class Image_CNN(Model):\n",
    "    def __init__(self):\n",
    "        super(Image_CNN, self).__init__()\n",
    "        self.conv1 = Conv2D(conv1_filters, 5, input_shape= (..., 1), strides = 1, activation = 'relu')\n",
    "        self.conv2 = Conv2D(conv2_filters, 5, strides = 1, activation = 'relu')        \n",
    "        self.pool1 = MaxPool2D(pool_size = (2,2))\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.dropout = Dropout(rate = 0.4)        \n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(fc1_units, activation = 'relu')\n",
    "        self.d2 = Dense(fc2_units, activation = 'relu')        \n",
    "        self.dsoftmax = Dense(num_classes, activation = 'softmax')\n",
    "  \n",
    "    def call(self, x, training = False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training = training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        x = self.dsoftmax(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training = True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training = True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function        \n",
    "def test_step(images, labels):\n",
    "    # training = False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training = False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuff_size = int(0.25 * len(y_train))\n",
    "batch_size = 32\n",
    "    \n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(shuff_size).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Image_CNN()\n",
    "\n",
    "# Instantiate our neural network model from the predefined class. Also define the loss function and optimizer.\n",
    "model = Image_CNN()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define the metrics for loss and accuracy.\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')\n",
    "\n",
    "    # Run and iterate model over epochs\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    # Train then test the model\n",
    "    for images, labels in train_data:\n",
    "        train_step(images, labels)\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    # Print results\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100))\n",
    "print(\"time elapsed: {:.2f}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-portable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
